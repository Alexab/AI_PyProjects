"""
ссылка:
http://deep.uran.ru/wiki/index.php?title=%D0%AD%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D1%8B_%D1%81_%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D1%8B%D0%BC_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD%D0%BE%D0%BC_%D0%B2_Keras

Keras - надстройка над Theano, предназначенная для работы с нейронными сетями.
Официальная страница: http://keras.io

В Keras входят базы примеров, например, MNIST (рукописные символы). Поэтому Keras является хорошим средством
для первых практических экспериментов с нейронными сетями.
(Также Keras позволяет работать с TensorFlow).
"""

import numpy as np

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense
from keras.utils import np_utils

from keras.models import Sequential
from keras.layers.core import Dense

""" 1)  Загрузка обучающих и тестовых примеров из MNIST.
        Обратите внимание на приведение значений яркостей из диапазона 0..255 в [0,1]:
            X_train /= 255
            X_test  /= 255 
"""
nb_classes = 10

# the data, shuffled and split between tran and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.reshape(60000, 784)
X_test  = X_test.reshape(10000, 784)
X_train = X_train.astype("float32")
X_test  = X_test.astype("float32")
X_train /= 255
X_test  /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0],  'test samples')

# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
print(Y_train[0])

""" 2)  Создание сети - однослойного перцептрона
        - Входные примеры базы MNIST являются изображениями 28x28 пикселей, которые преобразованы в вектора длины 784. 
        - Мы создаем слой, состоящий из 10 нейронов, в котором каждый нейрон связян со всеми входами. 
          Такие слои в Keras называются "Dense". 
        - Мы задаём функцию активации выходного слоя - softmax, 
          которая применяется для задач классификации. То есть, на выходе будет выдаваться 10 неотрицательных чисел, 
          сумма которых равна 1, которые характеризуют вероятности того, что входное изображение является цифрой 0,1,...9.
"""
model = Sequential()
#для однослойного
#model.add(Dense(input_dim=784, activation="softmax", units=nb_classes))
#для двуслойного
#model.add(Dense(input_dim=784, activation='relu', units=100))
#model.add(Dense(units=nb_classes, input_dim=784, activation='softmax'))
#для трехслойного
model.add(Dense(activation='relu', units=100, input_dim=784))
model.add(Dense(activation='relu', units=200))
model.add(Dense(activation='softmax', units=nb_classes))

""" 3)  Функция model.summary() выдаст информацию о слоях сети и общем числе параметров: 

            Layer (type)                     Output Shape          Param #     Connected to                     
            ====================================================================================================
            dense_8 (Dense)                  (None, 10)            7850        dense_input_5[0][0]              
            ====================================================================================================
            Total params: 7850
            
        То есть, у сети 7850 параметров, что складывается из 784*10 - веса от каждого входа к каждому нейрону, 
        плюс 10 значений сдвига (bias) (? требуется уточнение)
"""
model.summary() #Print model info

""" 4)  Компиляция модели
        Теперь, скомпилируем модель. Это значит, что мы построим функцию Python, которая позволит вычислять 
        результат работы сети на входном векторе, вызывая функцию model.predict(...). Но, главное, 
        посчитается функция вычисления градиента функции ошибки по весам сети, что необходимо для осуществления 
        обучения (настройки) параметров сети. (Эти вычисления делаются с помощью Theano).
        
        Здесь 'adam' - тип градиентного спуска, 'categorical_crossentropy' - функция штрафа, кроссэнтропия, 
        которую следует использовать для задач классификации, как у нас, metrics=['accuracy'] значит, что 
        мы будем вычислять в модели не только функцию штрафа, но и точность работы, то есть, число правильно 
        классифицированных примеров.
"""
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

""" 5)  Обучение сети
        Это самая ресурсоемкая операция. Мы выполняем обучение "пачками" (mini-batch) по batch_size=128 примеров. 
        При этом, осуществляет nb_epoch=5 проходов по всем входным примерам.

        Другие варианты режима обучения:
        - по одному примеру - медленная сходимость, и алгоритм "не видит" хороших минимумов,
        - по всем примерам сразу - трудоемко, сходимость к локальному минимуму и склонность к переобучению.

        А обработка mini-batch - промежуточный вариант, который называется градиентным стохастическим спуском
        
        ------------------------------------------------------------------------------------------------------------
        
        Выдастся информация о проходах обучения:

                Train on 60000 samples, validate on 10000 samples
                Epoch 1/5
                0s - loss: 0.2305 - acc: 0.9363 - val_loss: 0.2667 - val_acc: 0.9280
                Epoch 2/5
                0s - loss: 0.2298 - acc: 0.9366 - val_loss: 0.2691 - val_acc: 0.9276
                Epoch 3/5
                0s - loss: 0.2296 - acc: 0.9366 - val_loss: 0.2693 - val_acc: 0.9286
                Epoch 4/5
                0s - loss: 0.2296 - acc: 0.9367 - val_loss: 0.2688 - val_acc: 0.9289
                Epoch 5/5
                0s - loss: 0.2292 - acc: 0.9370 - val_loss: 0.2707 - val_acc: 0.9267
        
        Здесь loss - функция ошибки, acc - точность на обучающей выборке, val_loss и val_acc - на тестовой выборке.        
        Видим, что точность на тестовой выборке получилась val_acc: 0.9267, то есть, 92.67%. 
"""
model.fit(X_train, Y_train, batch_size=128, nb_epoch=20, verbose=2, validation_data=(X_test, Y_test))

""" 6)  Тестирование модели
        Хотя на этапе обучения выдавалась информация о точности на тестовой выборке, при использовании 
        готовой модели, считанной из файла, нужно уметь проверять ее точность работы (возможно, на других 
        тестовых выборках). Для этого следует применять функцию model.evaluate.
        
        Выдастся результат:
                ('Test accuracy:', 0.92669999999999997)
"""
score = model.evaluate(X_test, Y_test,verbose=0)
#print('Test score:', score[0])
print('Test accuracy:', score[1])

#Сохраним модель
model.save("my_model.h5")

""" Дополнительные возможности и манипуляции
        
    о   Запись и считывание из файла
        (https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)
        Модель можно записывать и считывать из файла в формате HDF5:
            model.save("my_model.h5") - запись в файл структуры модели и весов
            model = keras.models.load_model("my_model.h5") - считывание из файла структуры модели и весов и компиляция
            
    
    о   Увеличение числа прогонов
        Если осуществить 20 обучающих прогонов (nb_epoch=20), то точность получится 92.62%. То есть, для однослойного 
        перцептрона увеличение числа прогонов не улучшает точность работы (была 92.66%).
        
        
    о   Выключение нормализации входных данных
        Закомментируем нормализацию входных яркостей:
                #X_train /= 255
                #X_test  /= 255
        Тогда процесс обучение сходится гораздо хуже (была 92.66%):
        - на 5 обучающих прогонах точность 66.22%
        - на 20 обучающих прогонах точность 66.69%
        То есть, точность с 92% упала до 66%, при этом, увеличение количества прогонов не улучшает ситуацию.

        !!! Прежде чем двигаться дальше, раскомментируйте нормализацию входных яркостей обратно!
        
        
    о   Двуслойный перцептрон
        Создадим теперь сеть с двумя "плотными" слоями.
        - Первый слой ("скрытый слой") будет состоять из 100 нейронов, соединенных со всеми входами. Функция 
          активации будет 'relu' - это самая популярная функция активации для скрытых слоев.
        - Второй слой будет состоять из 10 нейронов с функцией активации 'softmax'.
                model = Sequential()
                model.add(Dense(output_dim=100, input_dim=784, activation='relu'))
                model.add(Dense(output_dim=nb_classes, activation='softmax'))

        В этом случае у сети будет уже 79510 параметров (в однослойном случае было 7850), а точность работы 
        существенно возрастет 97.07% (в однослойном случае была 92.66%).        
        Из теоремы Колмогорова следует, что любую функцию можно аппроксимировать с любой точностью, используя 
        двуслойную сеть, задав достаточно много нейронов в первом слое. Правда, эта теорема не говорит о том, 
        что обучение сети сойдется к нужной аппроксимации, но указывает, что теоретически, достаточно двух слоев.
        
        
    о   Трехслойный перцептрон
        Добавим еще один скрытый слой из 200 нейронов после первого слоя двуслойной сети
                model = Sequential()
                #model.add(Dense(output_dim=100, input_dim=784, activation='relu'))
                #model.add(Dense(output_dim=200, activation='relu'))
                #model.add(Dense(output_dim=nb_classes, activation='softmax'))

        В такой сети будет 100710 параметров, а точность работы 97.74%.                
"""

""" ЗАКЛЮЧЕНИЕ
    Мы получили следующие результаты:

    Однослойная сеть: 7850 параметров, точность 92.66%
    Двуслойная сеть: 79510 параметров, точность 97.07%
    Трехслойная сеть: 100710 параметров, точность 97.74%

    Из таблицы следует, что двуслойная "плотная" сеть работает существенно лучше однослойной, но трехслойная сеть 
    работает лишь немного лучше двухслойной. 
    Кроме того, при увеличении числа слоев может возникнуть эффект переобучения.

    Поэтому, дальнейшим шагом следует использовать не плотные, а сверточные слои. Они используют меньшее число 
    параметров и менее склонны к переобучению, а потому, из них можно строить более глубокие сети. 

"""